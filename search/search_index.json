{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"OVHcloud TechLabs - Workbooks","text":"<p>Welcome to the comprehensive collection of tutorials and workbooks for OVHcloud services.</p>"},{"location":"#available-workbooks","title":"Available Workbooks","text":""},{"location":"#public-cloud","title":"Public Cloud","text":""},{"location":"#ai-endpoints","title":"AI Endpoints","text":"<ul> <li>RAG Tutorial - Build lab-ready RAG systems using OVHcloud AI Endpoints</li> <li>VLM Tutorial - Interactive car verification using Vision Language Models</li> </ul>"},{"location":"#coming-soon","title":"Coming Soon","text":"<ul> <li>Object Storage tutorials</li> <li>Kubernetes workbooks</li> <li>Database guides</li> <li>Networking tutorials</li> </ul> <p>Browse All Workbooks</p>"},{"location":"en/","title":"OVHcloud TechLabs - Workbooks","text":"<p>Welcome to the comprehensive collection of tutorials and workbooks for OVHcloud services.</p>"},{"location":"en/#available-workbooks","title":"Available Workbooks","text":""},{"location":"en/#public-cloud","title":"Public Cloud","text":""},{"location":"en/#ai-endpoints","title":"AI Endpoints","text":"<ul> <li>RAG Tutorial - Build lab-ready RAG systems using OVHcloud AI Endpoints</li> </ul>"},{"location":"en/#coming-soon","title":"Coming Soon","text":"<ul> <li>Object Storage tutorials</li> <li>Kubernetes workbooks</li> <li>Database guides</li> </ul> <p>Browse All Workbooks</p>"},{"location":"en/public-cloud/","title":"Public Cloud Workbooks","text":"<p>Comprehensive tutorials for OVHcloud Public Cloud services.</p>"},{"location":"en/public-cloud/#ai-endpoints","title":"AI Endpoints","text":"<ul> <li>RAG Tutorial - Complete guide to Retrieval-Augmented Generation for testing</li> </ul>"},{"location":"en/public-cloud/#coming-soon","title":"Coming Soon","text":"<ul> <li>Compute tutorials</li> <li>Storage guides</li> <li>Networking workbooks</li> </ul>"},{"location":"en/public-cloud/ai-endpoints/","title":"AI Endpoints Workbooks","text":"<p>Learn to build AI applications using OVHcloud AI Endpoints.</p>"},{"location":"en/public-cloud/ai-endpoints/#available-tutorials","title":"Available Tutorials","text":""},{"location":"en/public-cloud/ai-endpoints/#rag-tutorial","title":"RAG Tutorial","text":"<p>Build Retrieval-Augmented Generation systems for testing with: - Step-by-step setup guide - Real OVHcloud integration - Performance optimization - Hands-on experiments</p> <p>Difficulty: Intermediate Time: 2-3 hours Prerequisites: Basic Python, OVHcloud account</p>"},{"location":"en/public-cloud/ai-endpoints/rag-tutorial/","title":"RAG with OVHcloud AI Endpoints Tutorial","text":"<p>Build Retrieval-Augmented Generation systems for testing using OVHcloud AI Endpoints.</p>"},{"location":"en/public-cloud/ai-endpoints/rag-tutorial/#what-youll-build","title":"What You'll Build","text":"<p>A complete RAG system that: - Connects to OVHcloud embedding and LLM APIs - Processes knowledge bases efficiently - Provides contextual, accurate responses - Handles test scenarios</p>"},{"location":"en/public-cloud/ai-endpoints/rag-tutorial/#learning-path","title":"Learning Path","text":"<ol> <li>Setup Guide - Complete step-by-step tutorial</li> <li>Download Scripts - All Python files</li> </ol>"},{"location":"en/public-cloud/ai-endpoints/rag-tutorial/#quick-start","title":"Quick Start","text":"<p>Before You Begin</p> <p>Make sure you have: - OVHcloud account with AI Endpoints access - Basic Python knowledge - Linux environment (Debian 12 recommended)</p> <p>Start Tutorial Download Scripts</p>"},{"location":"en/public-cloud/ai-endpoints/rag-tutorial/setup-guide/","title":"RAG Tutorial Setup Guide","text":"<p>About this guide</p> <p>This step-by-step tutorial will walk you through building a lab-ready RAG system using OVHcloud AI Endpoints. Follow each step carefully to ensure a successful setup.</p>"},{"location":"en/public-cloud/ai-endpoints/rag-tutorial/setup-guide/#prerequisites","title":"Prerequisites","text":"<ul> <li> OVHcloud account with AI Endpoints access</li> <li> Basic Python knowledge</li> <li> Linux environment (Debian 12 recommended)</li> </ul>"},{"location":"en/public-cloud/ai-endpoints/rag-tutorial/setup-guide/#step-1-update-system-and-install-python","title":"Step 1: Update System and Install Python","text":"<p>System Requirements</p> <p>This tutorial requires Python 3.11+ and pip. The commands below will ensure your system is up-to-date.</p> Debian/UbuntuCentOS/RHELmacOS <pre><code># Update package list and upgrade system\nsudo apt update &amp;&amp; sudo apt upgrade -y\n\n# Install Python 3.11+ and pip\nsudo apt install python3 python3-pip python3-venv curl -y\n\n# Verify Python version (should be 3.11+)\npython3 --version\n</code></pre> <pre><code># Update package list and upgrade system\nsudo yum update -y\n\n# Install Python 3.11+ and pip\nsudo yum install python3 python3-pip python3-venv curl -y\n\n# Verify Python version (should be 3.11+)\npython3 --version\n</code></pre> <pre><code># Install Homebrew if not already installed\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\n# Install Python 3.11+\nbrew install python@3.11\n\n# Verify Python version\npython3 --version\n</code></pre>"},{"location":"en/public-cloud/ai-endpoints/rag-tutorial/setup-guide/#step-2-get-ovhcloud-ai-endpoints-access-token","title":"Step 2: Get OVHcloud AI Endpoints Access Token","text":"<p>API Token Required</p> <p>This tutorial requires an OVHcloud AI Endpoints access token. Without this token, you won't be able to access the embedding and LLM APIs.</p>"},{"location":"en/public-cloud/ai-endpoints/rag-tutorial/setup-guide/#access-token-steps","title":"Access Token Steps","text":"<ul> <li> Go to OVHcloud AI Endpoints</li> <li> Create an account or log in to your existing account</li> <li> Navigate to Public Cloud in the dashboard</li> <li> Create a new Public Cloud Project or select an existing one</li> <li> Navigate to AI Endpoints \u2192 API keys section</li> <li> Click on Create a new API key button</li> <li> Copy the generated token for use in this tutorial</li> </ul> <p>Token Security</p> <p>Your API token is sensitive information. Never share it publicly or commit it to version control systems. We'll use environment variables to keep it secure.</p> <pre><code>flowchart TD\n    A[Start] --&gt; B[Create OVHcloud Account]\n    B --&gt; C[Navigate to Public Cloud]\n    C --&gt; D[Select/Create Project]\n    D --&gt; E[Go to AI Endpoints]\n    E --&gt; F[Create API Key]\n    F --&gt; G[Copy Token]\n    G --&gt; H[Use in Tutorial]\n    style F fill:#f96,stroke:#333,stroke-width:2px\n    style G fill:#f96,stroke:#333,stroke-width:2px</code></pre>"},{"location":"en/public-cloud/ai-endpoints/rag-tutorial/setup-guide/#step-3-create-project-directory-and-virtual-environment","title":"Step 3: Create Project Directory and Virtual Environment","text":"<pre><code># Create project directory\nmkdir ~/rag-ovh-test\ncd ~/rag-ovh-test\n\n# Create virtual environment\npython3 -m venv venv\n\n# Activate virtual environment\nsource venv/bin/activate\n\n# Upgrade pip\npip install --upgrade pip\n</code></pre>"},{"location":"en/public-cloud/ai-endpoints/rag-tutorial/setup-guide/#step-4-install-required-dependencies","title":"Step 4: Install Required Dependencies","text":"<pre><code># Install ALL required dependencies for OVHcloud integration\npip install langchain langchain-community faiss-cpu requests openai python-dotenv numpy\n\n# Verify installations\npip list | grep -E \"(langchain|faiss|requests|openai|numpy)\"\n</code></pre> <p>You should see output like:</p> <pre><code>faiss-cpu                1.11.0\nlangchain                0.3.25\nlangchain-community      0.3.24\nlangchain-core           0.3.61\nlangchain-text-splitters 0.3.8\nnumpy                    2.2.6\nopenai                   1.82.0\nrequests                 2.32.3\nrequests-toolbelt        1.0.0\n</code></pre>"},{"location":"en/public-cloud/ai-endpoints/rag-tutorial/setup-guide/#step-5-set-up-environment-variables","title":"Step 5: Set Up Environment Variables","text":"<pre><code># Create environment file\nnano .env\n</code></pre> <p>Add your actual OVHcloud token:</p> <pre><code>OVH_AI_ENDPOINTS_ACCESS_TOKEN=your_actual_token_here\n</code></pre>"},{"location":"en/public-cloud/ai-endpoints/rag-tutorial/setup-guide/#step-6-test-ovhcloud-api-connectivity","title":"Step 6: Test OVHcloud API Connectivity","text":"<pre><code>nano test_ovh_connection.py\n</code></pre> <pre><code>import os\nimport requests\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\n\ndef test_ovh_embedding_api():\n    \"\"\"Test OVHcloud embedding API connectivity\"\"\"\n    token = os.getenv(\"OVH_AI_ENDPOINTS_ACCESS_TOKEN\")\n\n    if not token:\n        print(\"\u274c No token found. Check your .env file.\")\n        return False, None\n\n    # Test the three OVHcloud embedding models with correct URLs from documentation\n    models_to_test = [\n        {\n            \"name\": \"bge-multilingual-gemma2\",\n            \"url\": \"https://bge-multilingual-gemma2.endpoints.kepler.ai.cloud.ovh.net/api/text2vec\",\n            \"dimensions\": 3584\n        },\n        {\n            \"name\": \"bge-base-en-v1.5\", \n            \"url\": \"https://bge-base-en-v1-5.endpoints.kepler.ai.cloud.ovh.net/api/text2vec\",\n            \"dimensions\": 768\n        },\n        {\n            \"name\": \"bge-m3\",\n            \"url\": \"https://bge-m3.endpoints.kepler.ai.cloud.ovh.net/api/text2vec\", \n            \"dimensions\": 1024\n        }\n    ]\n\n    test_text = \"This is a test sentence for embedding.\"\n\n    for model in models_to_test:\n        try:\n            print(f\"Testing {model['name']}...\")\n            response = requests.post(\n                model[\"url\"],\n                data=test_text,\n                headers={\n                    \"Content-Type\": \"text/plain\",\n                    \"Authorization\": f\"Bearer {token}\"\n                },\n                timeout=30\n            )\n\n            if response.status_code == 200:\n                embedding = response.json()\n                print(f\"\u2705 {model['name']} works! Dimensions: {len(embedding)} (expected: {model['dimensions']})\")\n                print(f\"First 5 values: {embedding[:5]}\")\n                return True, model  # Return the working model\n            else:\n                print(f\"\u274c {model['name']} failed: {response.status_code}\")\n                print(f\"Response: {response.text}\")\n\n        except Exception as e:\n            print(f\"\u274c {model['name']} connection error: {e}\")\n\n    return False, None\n\ndef test_ovh_llm_api():\n    \"\"\"Test OVHcloud LLM API connectivity\"\"\"\n    from openai import OpenAI\n\n    token = os.getenv(\"OVH_AI_ENDPOINTS_ACCESS_TOKEN\")\n\n    try:\n        client = OpenAI(\n            base_url=\"https://oai.endpoints.kepler.ai.cloud.ovh.net/v1\",\n            api_key=token\n        )\n\n        response = client.chat.completions.create(\n            model=\"Meta-Llama-3_3-70B-Instruct\",\n            messages=[{\"role\": \"user\", \"content\": \"Say hello in one sentence.\"}],\n            temperature=0,\n            max_tokens=50\n        )\n\n        print(f\"\u2705 LLM API works! Response: {response.choices[0].message.content}\")\n        return True\n\n    except Exception as e:\n        print(f\"\u274c LLM API failed: {e}\")\n        return False\n\nif __name__ == \"__main__\":\n    print(\"Testing OVHcloud AI Endpoints connectivity...\\n\")\n\n    embedding_ok, working_model = test_ovh_embedding_api()\n    print()\n    llm_ok = test_ovh_llm_api()\n\n    if embedding_ok and llm_ok:\n        print(f\"\\n\ud83c\udf89 All OVHcloud APIs are working!\")\n        print(f\"\u2705 Working embedding model: {working_model['name']}\")\n        print(\"You can proceed with RAG testing.\")\n    else:\n        print(\"\\n\u26a0\ufe0f  Some APIs failed. Check your token and try again.\")\n</code></pre>"},{"location":"en/public-cloud/ai-endpoints/rag-tutorial/setup-guide/#step-7-run-api-connectivity-test","title":"Step 7: Run API Connectivity Test","text":"<pre><code># Run the connection test\npython test_ovh_connection.py\n</code></pre> <p>You should see output like:</p> <pre><code>Testing bge-multilingual-gemma2...\n\u2705 bge-multilingual-gemma2 works! Dimensions: 3584 (expected: 3584)\nFirst 5 values: [-0.63427734, -1.53125, 5.6796875, -4.9648438, 0.7944336]\n\u2705 LLM API works! Response: Hello, it's nice to meet you and I'm here to help with any questions or topics you'd like to discuss.\n</code></pre> <p>Note: The negative values in embeddings are normal - they represent coordinates in high-dimensional space and can be positive or negative.</p>"},{"location":"en/public-cloud/ai-endpoints/rag-tutorial/setup-guide/#step-8-create-full-rag-test-script","title":"Step 8: Create Full RAG Test Script","text":"<pre><code>nano test_rag_ovh.py\n</code></pre> <pre><code>import os\nimport requests\nfrom langchain.schema import Document\nfrom langchain_community.vectorstores import FAISS  # Updated import\nfrom langchain.embeddings.base import Embeddings\nfrom langchain.llms.base import LLM\nfrom langchain.chains import RetrievalQA\nfrom typing import List\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\n\nclass OVHEmbeddings(Embeddings):\n    \"\"\"OVHcloud embeddings wrapper for LangChain.\"\"\"\n\n    def __init__(self, model_name=\"bge-multilingual-gemma2\"):\n        self.token = os.getenv(\"OVH_AI_ENDPOINTS_ACCESS_TOKEN\")\n\n        # Model configurations\n        self.models = {\n            \"bge-multilingual-gemma2\": {\n                \"url\": \"https://bge-multilingual-gemma2.endpoints.kepler.ai.cloud.ovh.net/api/text2vec\",\n                \"dimensions\": 3584\n            },\n            \"bge-base-en-v1.5\": {\n                \"url\": \"https://bge-base-en-v1-5.endpoints.kepler.ai.cloud.ovh.net/api/text2vec\",\n                \"dimensions\": 768\n            },\n            \"bge-m3\": {\n                \"url\": \"https://bge-m3.endpoints.kepler.ai.cloud.ovh.net/api/text2vec\",\n                \"dimensions\": 1024\n            }\n        }\n\n        if model_name not in self.models:\n            raise ValueError(f\"Model {model_name} not supported. Available: {list(self.models.keys())}\")\n\n        self.model_name = model_name\n        self.url = self.models[model_name][\"url\"]\n\n    def embed_documents(self, texts: List[str]) -&gt; List[List[float]]:\n        embeddings = []\n        for text in texts:\n            response = requests.post(\n                self.url,\n                data=text,\n                headers={\n                    \"Content-Type\": \"text/plain\",\n                    \"Authorization\": f\"Bearer {self.token}\"\n                }\n            )\n            if response.status_code == 200:\n                embeddings.append(response.json())\n            else:\n                raise Exception(f\"Embedding failed for model {self.model_name}: {response.status_code} - {response.text}\")\n        return embeddings\n\n    def embed_query(self, text: str) -&gt; List[float]:\n        return self.embed_documents([text])[0]\n\nclass OVHLLM(LLM):\n    \"\"\"OVHcloud LLM wrapper for LangChain.\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        token = os.getenv(\"OVH_AI_ENDPOINTS_ACCESS_TOKEN\")\n        if not token:\n            raise ValueError(\"OVH_AI_ENDPOINTS_ACCESS_TOKEN environment variable is required\")\n\n        try:\n            from openai import OpenAI\n            self._client = OpenAI(\n                base_url=\"https://oai.endpoints.kepler.ai.cloud.ovh.net/v1\",\n                api_key=token\n            )\n        except Exception as e:\n            raise Exception(f\"Failed to setup OpenAI client: {e}\")\n\n    def _call(self, prompt: str, stop=None) -&gt; str:\n        try:\n            response = self._client.chat.completions.create(\n                model=\"Meta-Llama-3_3-70B-Instruct\",\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                temperature=0,\n                max_tokens=1024\n            )\n            return response.choices[0].message.content\n        except Exception as e:\n            raise Exception(f\"LLM call failed: {e}\")\n\n    @property\n    def _llm_type(self) -&gt; str:\n        return \"ovh_llm\"\n\ndef setup_rag_system():\n    \"\"\"Set up the complete RAG system with LangChain.\"\"\"\n\n    print(\"\ud83d\udd27 Setting up RAG system...\")\n\n    # Create knowledge base\n    chunks = [\n        Document(\n            page_content=\"Export crashes occur with files over 4GB on Mac due to memory limits. This affects ProRes format specifically.\",\n            metadata={\"source\": \"troubleshooting_guide\", \"chunk_id\": 1}\n        ),\n        Document(\n            page_content=\"Solution for 4GB+ export crashes: Enable compression in export settings, then switch from ProRes to H.264 codec. This reduces memory usage by 60%.\",\n            metadata={\"source\": \"troubleshooting_guide\", \"chunk_id\": 2}\n        ),\n        Document(\n            page_content=\"Windows users experiencing export issues should update graphics drivers to latest version. Download from manufacturer website.\",\n            metadata={\"source\": \"troubleshooting_guide\", \"chunk_id\": 3}\n        )\n    ]\n\n    # Build vector store with similarity threshold\n    print(\"\ud83d\udcca Creating embeddings...\")\n    # Use bge-multilingual-gemma2 since it worked in your connection test\n    embeddings = OVHEmbeddings(model_name=\"bge-multilingual-gemma2\")\n    vectorstore = FAISS.from_documents(chunks, embeddings)\n\n    # Use regular similarity search instead of score threshold (due to embedding scoring issues)\n    retriever = vectorstore.as_retriever(\n        search_type=\"similarity\",\n        search_kwargs={\"k\": 3}  # Get top 3 results\n    )\n\n    # Create RAG chain\n    print(\"\ud83e\udd16 Setting up LLM...\")\n    llm = OVHLLM()\n    rag_chain = RetrievalQA.from_chain_type(\n        llm=llm,\n        chain_type=\"stuff\",\n        retriever=retriever,\n        return_source_documents=True\n    )\n\n    print(\"\u2705 RAG system ready!\")\n    return rag_chain\n\ndef main():\n    try:\n        rag_system = setup_rag_system()\n\n        # Test queries\n        test_queries = [\n            \"My video export keeps crashing\",\n            \"How do I fix export crashes on Mac?\",\n            \"Why does my coffee taste bitter?\"  # No-match scenario\n        ]\n\n        for question in test_queries:\n            print(f\"\\n{'='*60}\")\n            print(f\"\u2753 Question: {question}\")\n\n            # Use the new invoke method instead of deprecated __call__\n            result = rag_system.invoke({\"query\": question})\n            print(f\"\ud83d\udcac Answer: {result['result']}\")\n\n            if result.get('source_documents'):\n                sources = [f\"Chunk {doc.metadata['chunk_id']}\" \n                          for doc in result['source_documents']]\n                print(f\"\ud83d\udcda Sources: {', '.join(sources)}\")\n            else:\n                print(\"\ud83d\udcda Sources: No matching sources found\")\n\n    except Exception as e:\n        print(f\"\u274c Error: {e}\")\n        print(\"Make sure your OVH_AI_ENDPOINTS_ACCESS_TOKEN is correct in .env file\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"en/public-cloud/ai-endpoints/rag-tutorial/setup-guide/#step-9-run-full-rag-test","title":"Step 9: Run Full RAG Test","text":"<pre><code># Run the complete RAG system test\npython test_rag_ovh.py\n</code></pre> <p>Expected output:</p> <pre><code>\ud83d\udd27 Setting up RAG system...\n\ud83d\udcca Creating embeddings...\n\ud83e\udd16 Setting up LLM...\n\u2705 RAG system ready!\n\n============================================================\n\u2753 Question: My video export keeps crashing\n\ud83d\udcac Answer: This sounds like the known 4GB file issue on Mac systems...\n\ud83d\udcda Sources: Chunk 1, Chunk 2\n</code></pre>"},{"location":"en/public-cloud/ai-endpoints/rag-tutorial/setup-guide/#step-10-performance-testing","title":"Step 10: Performance Testing","text":"<p>Create and run a comprehensive performance test that compares all three embedding models:</p> <pre><code>nano performance_test.py\n</code></pre> <pre><code>import time\nfrom test_rag_ovh import OVHEmbeddings, OVHLLM, Document\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.chains import RetrievalQA\n\ndef test_model_performance(model_name):\n    \"\"\"Test performance for a specific embedding model\"\"\"\n    print(f\"\\n\ud83e\uddea Testing {model_name}...\")\n\n    # Create knowledge base\n    chunks = [\n        Document(\n            page_content=\"Export crashes occur with files over 4GB on Mac due to memory limits. This affects ProRes format specifically.\",\n            metadata={\"source\": \"troubleshooting_guide\", \"chunk_id\": 1}\n        ),\n        Document(\n            page_content=\"Solution for 4GB+ export crashes: Enable compression in export settings, then switch from ProRes to H.264 codec. This reduces memory usage by 60%.\",\n            metadata={\"source\": \"troubleshooting_guide\", \"chunk_id\": 2}\n        ),\n        Document(\n            page_content=\"Windows users experiencing export issues should update graphics drivers to latest version. Download from manufacturer website.\",\n            metadata={\"source\": \"troubleshooting_guide\", \"chunk_id\": 3}\n        )\n    ]\n\n    # Setup timing\n    start_time = time.time()\n\n    # Create embeddings and vector store\n    embeddings = OVHEmbeddings(model_name=model_name)\n    vectorstore = FAISS.from_documents(chunks, embeddings)\n    retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n\n    # Create RAG chain\n    llm = OVHLLM()\n    rag_chain = RetrievalQA.from_chain_type(\n        llm=llm,\n        chain_type=\"stuff\",\n        retriever=retriever,\n        return_source_documents=True\n    )\n\n    setup_time = time.time() - start_time\n    print(f\"\u23f1\ufe0f  Setup time: {setup_time:.2f} seconds\")\n\n    # Test queries\n    test_query = \"My video export keeps crashing\"\n\n    start_time = time.time()\n    result = rag_chain.invoke({\"query\": test_query})\n    query_time = time.time() - start_time\n\n    print(f\"\u23f1\ufe0f  Query time: {query_time:.2f} seconds\")\n    print(f\"\ud83d\udcac Answer: {result['result']}\")\n\n    return setup_time, query_time\n\ndef test_performance():\n    \"\"\"Test performance across all embedding models\"\"\"\n    print(\"\ud83d\ude80 Starting comprehensive performance test...\")\n\n    models = [\n        \"bge-base-en-v1.5\",      # 768 dimensions\n        \"bge-m3\",                # 1024 dimensions  \n        \"bge-multilingual-gemma2\" # 3584 dimensions\n    ]\n\n    results = {}\n\n    for model in models:\n        try:\n            setup_time, query_time = test_model_performance(model)\n            results[model] = {\"setup\": setup_time, \"query\": query_time}\n        except Exception as e:\n            print(f\"\u274c {model} failed: {e}\")\n            results[model] = {\"setup\": None, \"query\": None}\n\n    # Summary\n    print(f\"\\n{'='*60}\")\n    print(\"\ud83d\udcca PERFORMANCE SUMMARY\")\n    print(f\"{'='*60}\")\n    print(f\"{'Model':&lt;25} {'Dimensions':&lt;12} {'Setup':&lt;10} {'Query':&lt;10}\")\n    print(\"-\" * 60)\n\n    model_dims = {\"bge-base-en-v1.5\": 768, \"bge-m3\": 1024, \"bge-multilingual-gemma2\": 3584}\n\n    for model, times in results.items():\n        dims = model_dims[model]\n        setup = f\"{times['setup']:.2f}s\" if times['setup'] else \"Failed\"\n        query = f\"{times['query']:.2f}s\" if times['query'] else \"Failed\"\n        print(f\"{model:&lt;25} {dims:&lt;12} {setup:&lt;10} {query:&lt;10}\")\n\n    print(f\"\\n\ud83d\udca1 Performance Insights:\")\n    print(\"   \u2022 Higher dimensions = better accuracy, slower performance\")\n    print(\"   \u2022 Lower dimensions = faster performance, potentially less accuracy\")\n    print(\"   \u2022 Choose based on your speed vs accuracy requirements\")\n\nif __name__ == \"__main__\":\n    test_performance()\n</code></pre> <p>Run the performance test:</p> <pre><code>python performance_test.py\n</code></pre> <p>Expected output:</p> <pre><code>\ud83d\ude80 Starting comprehensive performance test...\n\n\ud83e\uddea Testing bge-base-en-v1.5...\n\u23f1\ufe0f  Setup time: 0.85 seconds\n\u23f1\ufe0f  Query time: 1.45 seconds\n\ud83d\udcac Answer: [LLM response here]\n\n\ud83d\udcca PERFORMANCE SUMMARY\n============================================================\nModel                     Dimensions   Setup      Query     \n------------------------------------------------------------\nbge-base-en-v1.5         768          0.85s      1.45s     \nbge-m3                   1024         0.95s      1.60s     \nbge-multilingual-gemma2  3584         1.16s      1.99s\n</code></pre>"},{"location":"en/public-cloud/ai-endpoints/rag-tutorial/setup-guide/#step-11-test-ovhcloud-documentation-examples","title":"Step 11: Test OVHcloud Documentation Examples","text":"<p>Create and run a test that matches OVHcloud's official documentation:</p> <pre><code>nano test_ovh_docs_example.py\n</code></pre> <pre><code>import os\nimport requests\nimport numpy as np\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\ndef test_ovh_docs_examples():\n    \"\"\"Test the exact examples from OVHcloud documentation\"\"\"\n\n    # Test data from OVHcloud docs\n    text = [\n        \"Paris is the capital of France\",\n        \"Paris is the capital of France\", \n        \"Berlin is the capital of Germany\",\n        \"This endpoint converts input sentence into a vector embeddings\"\n    ]\n\n    headers = {\n        \"Content-Type\": \"text/plain\",\n        \"Authorization\": f\"Bearer {os.getenv('OVH_AI_ENDPOINTS_ACCESS_TOKEN')}\",\n    }\n\n    # Test all three models from documentation\n    models = [\n        (\"bge-multilingual-gemma2\", \"https://bge-multilingual-gemma2.endpoints.kepler.ai.cloud.ovh.net/api/text2vec\"),\n        (\"bge-m3\", \"https://bge-m3.endpoints.kepler.ai.cloud.ovh.net/api/text2vec\"),\n        (\"bge-base-en-v1.5\", \"https://bge-base-en-v1-5.endpoints.kepler.ai.cloud.ovh.net/api/text2vec\")\n    ]\n\n    # Cosine similarity function from docs\n    def cosine_similarity(vec_source, vec_compare):\n        return np.dot(vec_source, vec_compare)\n\n    for model_name, url in models:\n        print(f\"\\n\ud83e\uddea Testing {model_name}...\")\n        print(f\"URL: {url}\")\n\n        response_data = []\n        sentence_similarity = {}\n\n        for s in range(len(text)):\n            # Generate embeddings vector\n            response = requests.post(url, data=text[s], headers=headers)\n            if response.status_code == 200:\n                response_data.append(response.json())\n                if s &gt; 0:\n                    # Calculate sentence similarity\n                    similarity = cosine_similarity(response_data[0], response_data[s])\n                    sentence_similarity[f\"Similarity with sentence n\u00b0{s}\"] = \"{:.3f}\".format(similarity)\n            else:\n                print(f\"\u274c Error: {response.status_code}\")\n                print(f\"Response: {response.text}\")\n                break\n\n        if response_data:\n            print(f\"\u2705 {model_name} working!\")\n            print(f\"\ud83d\udcca Embedding dimensions: {len(response_data[0])}\")\n            print(f\"\ud83d\udd0d Similarities: {sentence_similarity}\")\n            return model_name, url  # Return first working model\n        else:\n            print(f\"\u274c {model_name} failed\")\n\n    return None, None\n\nif __name__ == \"__main__\":\n    print(\"Testing OVHcloud documentation examples...\")\n    working_model, working_url = test_ovh_docs_examples()\n\n    if working_model:\n        print(f\"\\n\ud83c\udf89 Success! Use {working_model} for RAG testing.\")\n        print(f\"Working URL: {working_url}\")\n    else:\n        print(\"\\n\u274c All models failed. Check your token.\")\n</code></pre> <p>Run this test:</p> <pre><code>python test_ovh_docs_example.py\n</code></pre> <p>You should see output showing successful embedding generation and similarity calculations.</p>"},{"location":"en/public-cloud/ai-endpoints/rag-tutorial/setup-guide/#step-12-experiment-and-customize","title":"Step 12: Experiment and Customize","text":"<p>Now that everything is working, try customizing the scripts to deepen your understanding:</p>"},{"location":"en/public-cloud/ai-endpoints/rag-tutorial/setup-guide/#a-test-different-knowledge-bases","title":"A. Test Different Knowledge Bases","text":"<p>Edit <code>test_rag_ovh.py</code> and replace the chunks with your own content:</p> <pre><code>nano test_rag_ovh.py\n</code></pre> <p>Find the <code>chunks = [...]</code> section and replace with different knowledge:</p> <pre><code>chunks = [\n    Document(\n        page_content=\"OVHcloud is a European cloud provider that keeps your data sovereign - meaning your data stays where you put it, not scattered across unknown servers in foreign countries. Perfect for companies who like to know where their bits and bytes are vacationing!\",\n        metadata={\"source\": \"ovh_facts\", \"chunk_id\": 1}\n    ),\n    Document(\n        page_content=\"OVHcloud offers predictable pricing with no surprise bills at month-end. Unlike some cloud providers who charge you for breathing near their servers, OVHcloud tells you upfront what you'll pay. It's like having a honest mechanic, but for cloud computing!\",\n        metadata={\"source\": \"ovh_facts\", \"chunk_id\": 2}\n    ),\n    Document(\n        page_content=\"OVHcloud uses water cooling in their standard server design, making their data centers more efficient than air-cooled alternatives. They're literally keeping it cool while keeping costs down - talk about a refreshing approach to cloud infrastructure!\",\n        metadata={\"source\": \"ovh_facts\", \"chunk_id\": 3}\n    ),\n    Document(\n        page_content=\"OVHcloud's Go Global initiative spans 4 continents with data centers worldwide, yet maintains local presence and support. They're globally distributed but locally rooted - like a multinational company that still remembers your birthday!\",\n        metadata={\"source\": \"ovh_facts\", \"chunk_id\": 4}\n    )\n]\n</code></pre>"},{"location":"en/public-cloud/ai-endpoints/rag-tutorial/setup-guide/#b-try-different-questions","title":"B. Try Different Questions","text":"<p>Modify the <code>test_queries</code> list to ask questions relevant to your new knowledge base:</p> <pre><code>test_queries = [\n    \"What makes OVHcloud different for data sovereignty?\",\n    \"How does OVHcloud keep pricing predictable?\",\n    \"Tell me about OVHcloud's cooling technology\",\n    \"How do I bake a perfect souffl\u00e9?\"  # Irrelevant question to test boundaries\n]\n</code></pre>"},{"location":"en/public-cloud/ai-endpoints/rag-tutorial/setup-guide/#c-test-different-models","title":"C. Test Different Models","text":"<p>Create a quick model comparison script:</p> <pre><code>nano model_comparison.py\n</code></pre> <pre><code>from test_rag_ovh import OVHEmbeddings\nimport requests\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\ndef compare_embeddings():\n    \"\"\"Compare how different models embed the same text\"\"\"\n    test_text = \"OVHcloud provides data sovereignty with predictable pricing and innovative water cooling technology.\"\n\n    models = [\"bge-base-en-v1.5\", \"bge-m3\", \"bge-multilingual-gemma2\"]\n\n    for model in models:\n        try:\n            embeddings = OVHEmbeddings(model_name=model)\n            result = embeddings.embed_query(test_text)\n            print(f\"{model}: {len(result)} dimensions, first 3 values: {result[:3]}\")\n        except Exception as e:\n            print(f\"{model}: Failed - {e}\")\n\nif __name__ == \"__main__\":\n    compare_embeddings()\n</code></pre> <pre><code>python model_comparison.py\n</code></pre>"},{"location":"en/public-cloud/ai-endpoints/rag-tutorial/setup-guide/#d-experiment-with-similarity-thresholds","title":"D. Experiment with Similarity Thresholds","text":"<p>Create a similarity experiment:</p> <pre><code>nano similarity_experiment.py\n</code></pre> <pre><code>from langchain_community.vectorstores import FAISS\nfrom langchain.schema import Document\nfrom test_rag_ovh import OVHEmbeddings\n\ndef test_similarity_thresholds():\n    \"\"\"Experiment with different similarity approaches\"\"\"\n\n    # Create test documents about cloud providers\n    docs = [\n        Document(page_content=\"OVHcloud ensures data sovereignty by keeping your data in specific geographic regions with transparent policies\"),\n        Document(page_content=\"AWS offers global cloud services with pay-as-you-use pricing across multiple availability zones\"), \n        Document(page_content=\"Cats are domestic animals that make popular pets and enjoy playing with yarn\"),\n        Document(page_content=\"Dogs are loyal companion animals known for their intelligence and trainability\")\n    ]\n\n    embeddings = OVHEmbeddings(model_name=\"bge-base-en-v1.5\")\n    vectorstore = FAISS.from_documents(docs, embeddings)\n\n    query = \"Tell me about cloud data sovereignty\"\n\n    # Test different k values\n    for k in [1, 2, 3, 4]:\n        results = vectorstore.similarity_search(query, k=k)\n        print(f\"\\nTop {k} results for '{query}':\")\n        for i, doc in enumerate(results):\n            print(f\"  {i+1}. {doc.page_content}\")\n\nif __name__ == \"__main__\":\n    test_similarity_thresholds()\n</code></pre> <pre><code>python similarity_experiment.py\n</code></pre>"},{"location":"en/public-cloud/ai-endpoints/rag-tutorial/setup-guide/#e-create-your-own-rag-application","title":"E. Create Your Own RAG Application","text":"<p>Try building a simple RAG app for a cooking assistant:</p> <pre><code>nano my_rag_app.py\n</code></pre> <p>```python from test_rag_ovh import OVHEmbeddings, OVHLLM, Document from langchain_community.vectorstores import FAISS from langchain.chains import RetrievalQA</p> <p>def create_ovhcloud_rag():     \"\"\"Create a RAG system for OVHcloud information\"\"\"</p> <pre><code># OVHcloud knowledge base with fun facts\novhcloud_knowledge = [\n    Document(\n        page_content=\"OVHcloud pioneered using water cooling in standard server designs, making data centers 30% more energy efficient than traditional air cooling. They're literally keeping it cool while saving the planet - one server at a time!\",\n        metadata={\"source\": \"ovh_tech\", \"chunk_id\": 1}\n    ),\n    Document(\n        page_content=\"OVHcloud's pay-as-you-go model means you only pay for what you actually use, with per-second billing on many services.\n</code></pre>"},{"location":"public-cloud/","title":"Public Cloud Workbooks","text":"<p>Comprehensive tutorials for OVHcloud Public Cloud services.</p>"},{"location":"public-cloud/#ai-endpoints","title":"AI Endpoints","text":"<ul> <li>RAG Tutorial - Complete guide to Retrieval-Augmented Generation for testing</li> <li>VLM Tutorial - Interactive car verification using Vision Language Models</li> </ul>"},{"location":"public-cloud/#coming-soon","title":"Coming Soon","text":"<ul> <li>Compute tutorials</li> <li>Storage guides</li> <li>Networking workbooks</li> <li>Kubernetes tutorials</li> </ul>"},{"location":"public-cloud/ai-endpoints/","title":"AI Endpoints Workbooks","text":"<p>Learn to build AI applications using OVHcloud AI Endpoints.</p>"},{"location":"public-cloud/ai-endpoints/#available-tutorials","title":"Available Tutorials","text":""},{"location":"public-cloud/ai-endpoints/#rag-tutorial","title":"RAG Tutorial","text":"<p>Build Retrieval-Augmented Generation systems for testing with: - Step-by-step setup guide - Real OVHcloud integration - Performance optimization - Hands-on experiments</p> <p>Difficulty: Intermediate Time: 2-3 hours Prerequisites: Basic Python, OVHcloud account</p>"},{"location":"public-cloud/ai-endpoints/#vlm-tutorial-car-verification","title":"VLM Tutorial - Car Verification","text":"<p>Build an interactive car verification system using Vision Language Models: - Multi-modal AI fact-checking - Interactive Chainlit web application - Real-time image analysis - Verification use cases</p> <p>Difficulty: Intermediate Time: 1-2 hours Prerequisites: Basic Python, OVHcloud account, car photos</p>"},{"location":"public-cloud/ai-endpoints/#coming-soon","title":"Coming Soon","text":"<ul> <li>Multi-modal AI tutorials</li> <li>Fine-tuning guides</li> <li>Advanced prompt engineering</li> <li>Production deployment guides</li> </ul>"},{"location":"public-cloud/ai-endpoints/rag-tutorial/","title":"RAG with OVHcloud AI Endpoints Tutorial","text":"<p>Build Retrieval-Augmented Generation systems for testing using OVHcloud AI Endpoints.</p>"},{"location":"public-cloud/ai-endpoints/rag-tutorial/#what-youll-build","title":"What You'll Build","text":"<p>A complete RAG system that: - Connects to OVHcloud embedding and LLM APIs - Processes knowledge bases efficiently - Provides contextual, accurate responses - Handles test scenarios</p>"},{"location":"public-cloud/ai-endpoints/rag-tutorial/#learning-path","title":"Learning Path","text":"<ol> <li>Setup Guide - Complete step-by-step tutorial</li> <li>Download Scripts - All Python files</li> </ol>"},{"location":"public-cloud/ai-endpoints/rag-tutorial/#quick-start","title":"Quick Start","text":"<p>Before You Begin</p> <p>Make sure you have: - OVHcloud account with AI Endpoints access - Basic Python knowledge - Linux environment (Debian 12 recommended)</p> <p>Start Tutorial Download Scripts</p>"},{"location":"public-cloud/ai-endpoints/rag-tutorial/setup-guide/","title":"RAG Tutorial Setup Guide","text":"<p>About this guide</p> <p>This step-by-step tutorial will walk you through building a lab-ready RAG system using OVHcloud AI Endpoints. Follow each step carefully to ensure a successful setup.</p>"},{"location":"public-cloud/ai-endpoints/rag-tutorial/setup-guide/#prerequisites","title":"Prerequisites","text":"<ul> <li> OVHcloud account with AI Endpoints access</li> <li> Basic Python knowledge</li> <li> Linux environment (Debian 12 recommended)</li> </ul>"},{"location":"public-cloud/ai-endpoints/rag-tutorial/setup-guide/#step-1-update-system-and-install-python","title":"Step 1: Update System and Install Python","text":"<p>System Requirements</p> <p>This tutorial requires Python 3.11+ and pip. The commands below will ensure your system is up-to-date.</p> Debian/UbuntuCentOS/RHELmacOS <pre><code># Update package list and upgrade system\nsudo apt update &amp;&amp; sudo apt upgrade -y\n\n# Install Python 3.11+ and pip\nsudo apt install python3 python3-pip python3-venv curl -y\n\n# Verify Python version (should be 3.11+)\npython3 --version\n</code></pre> <pre><code># Update package list and upgrade system\nsudo yum update -y\n\n# Install Python 3.11+ and pip\nsudo yum install python3 python3-pip python3-venv curl -y\n\n# Verify Python version (should be 3.11+)\npython3 --version\n</code></pre> <pre><code># Install Homebrew if not already installed\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\n# Install Python 3.11+\nbrew install python@3.11\n\n# Verify Python version\npython3 --version\n</code></pre>"},{"location":"public-cloud/ai-endpoints/rag-tutorial/setup-guide/#step-2-get-ovhcloud-ai-endpoints-access-token","title":"Step 2: Get OVHcloud AI Endpoints Access Token","text":"<p>API Token Required</p> <p>This tutorial requires an OVHcloud AI Endpoints access token. Without this token, you won't be able to access the embedding and LLM APIs.</p>"},{"location":"public-cloud/ai-endpoints/rag-tutorial/setup-guide/#access-token-steps","title":"Access Token Steps","text":"<ul> <li> Go to OVHcloud AI Endpoints</li> <li> Create an account or log in to your existing account</li> <li> Navigate to Public Cloud in the dashboard</li> <li> Create a new Public Cloud Project or select an existing one</li> <li> Navigate to AI Endpoints \u2192 API keys section</li> <li> Click on Create a new API key button</li> <li> Copy the generated token for use in this tutorial</li> </ul> <p>Token Security</p> <p>Your API token is sensitive information. Never share it publicly or commit it to version control systems. We'll use environment variables to keep it secure.</p> <pre><code>flowchart TD\n    A[Start] --&gt; B[Create OVHcloud Account]\n    B --&gt; C[Navigate to Public Cloud]\n    C --&gt; D[Select/Create Project]\n    D --&gt; E[Go to AI Endpoints]\n    E --&gt; F[Create API Key]\n    F --&gt; G[Copy Token]\n    G --&gt; H[Use in Tutorial]\n    style F fill:#f96,stroke:#333,stroke-width:2px\n    style G fill:#f96,stroke:#333,stroke-width:2px</code></pre>"},{"location":"public-cloud/ai-endpoints/rag-tutorial/setup-guide/#step-3-create-project-directory-and-virtual-environment","title":"Step 3: Create Project Directory and Virtual Environment","text":"<pre><code># Create project directory\nmkdir ~/rag-ovh-test\ncd ~/rag-ovh-test\n\n# Create virtual environment\npython3 -m venv venv\n\n# Activate virtual environment\nsource venv/bin/activate\n\n# Upgrade pip\npip install --upgrade pip\n</code></pre>"},{"location":"public-cloud/ai-endpoints/rag-tutorial/setup-guide/#step-4-install-required-dependencies","title":"Step 4: Install Required Dependencies","text":"<pre><code># Install ALL required dependencies for OVHcloud integration\npip install langchain langchain-community faiss-cpu requests openai python-dotenv numpy\n\n# Verify installations\npip list | grep -E \"(langchain|faiss|requests|openai|numpy)\"\n</code></pre> <p>You should see output like:</p> <pre><code>faiss-cpu                1.11.0\nlangchain                0.3.25\nlangchain-community      0.3.24\nlangchain-core           0.3.61\nlangchain-text-splitters 0.3.8\nnumpy                    2.2.6\nopenai                   1.82.0\nrequests                 2.32.3\nrequests-toolbelt        1.0.0\n</code></pre>"},{"location":"public-cloud/ai-endpoints/rag-tutorial/setup-guide/#step-5-set-up-environment-variables","title":"Step 5: Set Up Environment Variables","text":"<pre><code># Create environment file\nnano .env\n</code></pre> <p>Add your actual OVHcloud token:</p> <pre><code>OVH_AI_ENDPOINTS_ACCESS_TOKEN=your_actual_token_here\n</code></pre>"},{"location":"public-cloud/ai-endpoints/rag-tutorial/setup-guide/#step-6-test-ovhcloud-api-connectivity","title":"Step 6: Test OVHcloud API Connectivity","text":"<pre><code>nano test_ovh_connection.py\n</code></pre> <pre><code>import os\nimport requests\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\n\ndef test_ovh_embedding_api():\n    \"\"\"Test OVHcloud embedding API connectivity\"\"\"\n    token = os.getenv(\"OVH_AI_ENDPOINTS_ACCESS_TOKEN\")\n\n    if not token:\n        print(\"\u274c No token found. Check your .env file.\")\n        return False, None\n\n    # Test the three OVHcloud embedding models with correct URLs from documentation\n    models_to_test = [\n        {\n            \"name\": \"bge-multilingual-gemma2\",\n            \"url\": \"https://bge-multilingual-gemma2.endpoints.kepler.ai.cloud.ovh.net/api/text2vec\",\n            \"dimensions\": 3584\n        },\n        {\n            \"name\": \"bge-base-en-v1.5\", \n            \"url\": \"https://bge-base-en-v1-5.endpoints.kepler.ai.cloud.ovh.net/api/text2vec\",\n            \"dimensions\": 768\n        },\n        {\n            \"name\": \"bge-m3\",\n            \"url\": \"https://bge-m3.endpoints.kepler.ai.cloud.ovh.net/api/text2vec\", \n            \"dimensions\": 1024\n        }\n    ]\n\n    test_text = \"This is a test sentence for embedding.\"\n\n    for model in models_to_test:\n        try:\n            print(f\"Testing {model['name']}...\")\n            response = requests.post(\n                model[\"url\"],\n                data=test_text,\n                headers={\n                    \"Content-Type\": \"text/plain\",\n                    \"Authorization\": f\"Bearer {token}\"\n                },\n                timeout=30\n            )\n\n            if response.status_code == 200:\n                embedding = response.json()\n                print(f\"\u2705 {model['name']} works! Dimensions: {len(embedding)} (expected: {model['dimensions']})\")\n                print(f\"First 5 values: {embedding[:5]}\")\n                return True, model  # Return the working model\n            else:\n                print(f\"\u274c {model['name']} failed: {response.status_code}\")\n                print(f\"Response: {response.text}\")\n\n        except Exception as e:\n            print(f\"\u274c {model['name']} connection error: {e}\")\n\n    return False, None\n\ndef test_ovh_llm_api():\n    \"\"\"Test OVHcloud LLM API connectivity\"\"\"\n    from openai import OpenAI\n\n    token = os.getenv(\"OVH_AI_ENDPOINTS_ACCESS_TOKEN\")\n\n    try:\n        client = OpenAI(\n            base_url=\"https://oai.endpoints.kepler.ai.cloud.ovh.net/v1\",\n            api_key=token\n        )\n\n        response = client.chat.completions.create(\n            model=\"Meta-Llama-3_3-70B-Instruct\",\n            messages=[{\"role\": \"user\", \"content\": \"Say hello in one sentence.\"}],\n            temperature=0,\n            max_tokens=50\n        )\n\n        print(f\"\u2705 LLM API works! Response: {response.choices[0].message.content}\")\n        return True\n\n    except Exception as e:\n        print(f\"\u274c LLM API failed: {e}\")\n        return False\n\nif __name__ == \"__main__\":\n    print(\"Testing OVHcloud AI Endpoints connectivity...\\n\")\n\n    embedding_ok, working_model = test_ovh_embedding_api()\n    print()\n    llm_ok = test_ovh_llm_api()\n\n    if embedding_ok and llm_ok:\n        print(f\"\\n\ud83c\udf89 All OVHcloud APIs are working!\")\n        print(f\"\u2705 Working embedding model: {working_model['name']}\")\n        print(\"You can proceed with RAG testing.\")\n    else:\n        print(\"\\n\u26a0\ufe0f  Some APIs failed. Check your token and try again.\")\n</code></pre>"},{"location":"public-cloud/ai-endpoints/rag-tutorial/setup-guide/#step-7-run-api-connectivity-test","title":"Step 7: Run API Connectivity Test","text":"<pre><code># Run the connection test\npython test_ovh_connection.py\n</code></pre> <p>You should see output like:</p> <pre><code>Testing bge-multilingual-gemma2...\n\u2705 bge-multilingual-gemma2 works! Dimensions: 3584 (expected: 3584)\nFirst 5 values: [-0.63427734, -1.53125, 5.6796875, -4.9648438, 0.7944336]\n\u2705 LLM API works! Response: Hello, it's nice to meet you and I'm here to help with any questions or topics you'd like to discuss.\n</code></pre> <p>Note: The negative values in embeddings are normal - they represent coordinates in high-dimensional space and can be positive or negative.</p>"},{"location":"public-cloud/ai-endpoints/rag-tutorial/setup-guide/#step-8-create-full-rag-test-script","title":"Step 8: Create Full RAG Test Script","text":"<pre><code>nano test_rag_ovh.py\n</code></pre> <pre><code>import os\nimport requests\nfrom langchain.schema import Document\nfrom langchain_community.vectorstores import FAISS  # Updated import\nfrom langchain.embeddings.base import Embeddings\nfrom langchain.llms.base import LLM\nfrom langchain.chains import RetrievalQA\nfrom typing import List\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\n\nclass OVHEmbeddings(Embeddings):\n    \"\"\"OVHcloud embeddings wrapper for LangChain.\"\"\"\n\n    def __init__(self, model_name=\"bge-multilingual-gemma2\"):\n        self.token = os.getenv(\"OVH_AI_ENDPOINTS_ACCESS_TOKEN\")\n\n        # Model configurations\n        self.models = {\n            \"bge-multilingual-gemma2\": {\n                \"url\": \"https://bge-multilingual-gemma2.endpoints.kepler.ai.cloud.ovh.net/api/text2vec\",\n                \"dimensions\": 3584\n            },\n            \"bge-base-en-v1.5\": {\n                \"url\": \"https://bge-base-en-v1-5.endpoints.kepler.ai.cloud.ovh.net/api/text2vec\",\n                \"dimensions\": 768\n            },\n            \"bge-m3\": {\n                \"url\": \"https://bge-m3.endpoints.kepler.ai.cloud.ovh.net/api/text2vec\",\n                \"dimensions\": 1024\n            }\n        }\n\n        if model_name not in self.models:\n            raise ValueError(f\"Model {model_name} not supported. Available: {list(self.models.keys())}\")\n\n        self.model_name = model_name\n        self.url = self.models[model_name][\"url\"]\n\n    def embed_documents(self, texts: List[str]) -&gt; List[List[float]]:\n        embeddings = []\n        for text in texts:\n            response = requests.post(\n                self.url,\n                data=text,\n                headers={\n                    \"Content-Type\": \"text/plain\",\n                    \"Authorization\": f\"Bearer {self.token}\"\n                }\n            )\n            if response.status_code == 200:\n                embeddings.append(response.json())\n            else:\n                raise Exception(f\"Embedding failed for model {self.model_name}: {response.status_code} - {response.text}\")\n        return embeddings\n\n    def embed_query(self, text: str) -&gt; List[float]:\n        return self.embed_documents([text])[0]\n\nclass OVHLLM(LLM):\n    \"\"\"OVHcloud LLM wrapper for LangChain.\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        token = os.getenv(\"OVH_AI_ENDPOINTS_ACCESS_TOKEN\")\n        if not token:\n            raise ValueError(\"OVH_AI_ENDPOINTS_ACCESS_TOKEN environment variable is required\")\n\n        try:\n            from openai import OpenAI\n            self._client = OpenAI(\n                base_url=\"https://oai.endpoints.kepler.ai.cloud.ovh.net/v1\",\n                api_key=token\n            )\n        except Exception as e:\n            raise Exception(f\"Failed to setup OpenAI client: {e}\")\n\n    def _call(self, prompt: str, stop=None) -&gt; str:\n        try:\n            response = self._client.chat.completions.create(\n                model=\"Meta-Llama-3_3-70B-Instruct\",\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                temperature=0,\n                max_tokens=1024\n            )\n            return response.choices[0].message.content\n        except Exception as e:\n            raise Exception(f\"LLM call failed: {e}\")\n\n    @property\n    def _llm_type(self) -&gt; str:\n        return \"ovh_llm\"\n\ndef setup_rag_system():\n    \"\"\"Set up the complete RAG system with LangChain.\"\"\"\n\n    print(\"\ud83d\udd27 Setting up RAG system...\")\n\n    # Create knowledge base\n    chunks = [\n        Document(\n            page_content=\"Export crashes occur with files over 4GB on Mac due to memory limits. This affects ProRes format specifically.\",\n            metadata={\"source\": \"troubleshooting_guide\", \"chunk_id\": 1}\n        ),\n        Document(\n            page_content=\"Solution for 4GB+ export crashes: Enable compression in export settings, then switch from ProRes to H.264 codec. This reduces memory usage by 60%.\",\n            metadata={\"source\": \"troubleshooting_guide\", \"chunk_id\": 2}\n        ),\n        Document(\n            page_content=\"Windows users experiencing export issues should update graphics drivers to latest version. Download from manufacturer website.\",\n            metadata={\"source\": \"troubleshooting_guide\", \"chunk_id\": 3}\n        )\n    ]\n\n    # Build vector store with similarity threshold\n    print(\"\ud83d\udcca Creating embeddings...\")\n    # Use bge-multilingual-gemma2 since it worked in your connection test\n    embeddings = OVHEmbeddings(model_name=\"bge-multilingual-gemma2\")\n    vectorstore = FAISS.from_documents(chunks, embeddings)\n\n    # Use regular similarity search instead of score threshold (due to embedding scoring issues)\n    retriever = vectorstore.as_retriever(\n        search_type=\"similarity\",\n        search_kwargs={\"k\": 3}  # Get top 3 results\n    )\n\n    # Create RAG chain\n    print(\"\ud83e\udd16 Setting up LLM...\")\n    llm = OVHLLM()\n    rag_chain = RetrievalQA.from_chain_type(\n        llm=llm,\n        chain_type=\"stuff\",\n        retriever=retriever,\n        return_source_documents=True\n    )\n\n    print(\"\u2705 RAG system ready!\")\n    return rag_chain\n\ndef main():\n    try:\n        rag_system = setup_rag_system()\n\n        # Test queries\n        test_queries = [\n            \"My video export keeps crashing\",\n            \"How do I fix export crashes on Mac?\",\n            \"Why does my coffee taste bitter?\"  # No-match scenario\n        ]\n\n        for question in test_queries:\n            print(f\"\\n{'='*60}\")\n            print(f\"\u2753 Question: {question}\")\n\n            # Use the new invoke method instead of deprecated __call__\n            result = rag_system.invoke({\"query\": question})\n            print(f\"\ud83d\udcac Answer: {result['result']}\")\n\n            if result.get('source_documents'):\n                sources = [f\"Chunk {doc.metadata['chunk_id']}\" \n                          for doc in result['source_documents']]\n                print(f\"\ud83d\udcda Sources: {', '.join(sources)}\")\n            else:\n                print(\"\ud83d\udcda Sources: No matching sources found\")\n\n    except Exception as e:\n        print(f\"\u274c Error: {e}\")\n        print(\"Make sure your OVH_AI_ENDPOINTS_ACCESS_TOKEN is correct in .env file\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"public-cloud/ai-endpoints/rag-tutorial/setup-guide/#step-9-run-full-rag-test","title":"Step 9: Run Full RAG Test","text":"<pre><code># Run the complete RAG system test\npython test_rag_ovh.py\n</code></pre> <p>Expected output:</p> <pre><code>\ud83d\udd27 Setting up RAG system...\n\ud83d\udcca Creating embeddings...\n\ud83e\udd16 Setting up LLM...\n\u2705 RAG system ready!\n\n============================================================\n\u2753 Question: My video export keeps crashing\n\ud83d\udcac Answer: This sounds like the known 4GB file issue on Mac systems...\n\ud83d\udcda Sources: Chunk 1, Chunk 2\n</code></pre>"},{"location":"public-cloud/ai-endpoints/rag-tutorial/setup-guide/#step-10-performance-testing","title":"Step 10: Performance Testing","text":"<p>Create and run a comprehensive performance test that compares all three embedding models:</p> <pre><code>nano performance_test.py\n</code></pre> <pre><code>import time\nfrom test_rag_ovh import OVHEmbeddings, OVHLLM, Document\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.chains import RetrievalQA\n\ndef test_model_performance(model_name):\n    \"\"\"Test performance for a specific embedding model\"\"\"\n    print(f\"\\n\ud83e\uddea Testing {model_name}...\")\n\n    # Create knowledge base\n    chunks = [\n        Document(\n            page_content=\"Export crashes occur with files over 4GB on Mac due to memory limits. This affects ProRes format specifically.\",\n            metadata={\"source\": \"troubleshooting_guide\", \"chunk_id\": 1}\n        ),\n        Document(\n            page_content=\"Solution for 4GB+ export crashes: Enable compression in export settings, then switch from ProRes to H.264 codec. This reduces memory usage by 60%.\",\n            metadata={\"source\": \"troubleshooting_guide\", \"chunk_id\": 2}\n        ),\n        Document(\n            page_content=\"Windows users experiencing export issues should update graphics drivers to latest version. Download from manufacturer website.\",\n            metadata={\"source\": \"troubleshooting_guide\", \"chunk_id\": 3}\n        )\n    ]\n\n    # Setup timing\n    start_time = time.time()\n\n    # Create embeddings and vector store\n    embeddings = OVHEmbeddings(model_name=model_name)\n    vectorstore = FAISS.from_documents(chunks, embeddings)\n    retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n\n    # Create RAG chain\n    llm = OVHLLM()\n    rag_chain = RetrievalQA.from_chain_type(\n        llm=llm,\n        chain_type=\"stuff\",\n        retriever=retriever,\n        return_source_documents=True\n    )\n\n    setup_time = time.time() - start_time\n    print(f\"\u23f1\ufe0f  Setup time: {setup_time:.2f} seconds\")\n\n    # Test queries\n    test_query = \"My video export keeps crashing\"\n\n    start_time = time.time()\n    result = rag_chain.invoke({\"query\": test_query})\n    query_time = time.time() - start_time\n\n    print(f\"\u23f1\ufe0f  Query time: {query_time:.2f} seconds\")\n    print(f\"\ud83d\udcac Answer: {result['result']}\")\n\n    return setup_time, query_time\n\ndef test_performance():\n    \"\"\"Test performance across all embedding models\"\"\"\n    print(\"\ud83d\ude80 Starting comprehensive performance test...\")\n\n    models = [\n        \"bge-base-en-v1.5\",      # 768 dimensions\n        \"bge-m3\",                # 1024 dimensions  \n        \"bge-multilingual-gemma2\" # 3584 dimensions\n    ]\n\n    results = {}\n\n    for model in models:\n        try:\n            setup_time, query_time = test_model_performance(model)\n            results[model] = {\"setup\": setup_time, \"query\": query_time}\n        except Exception as e:\n            print(f\"\u274c {model} failed: {e}\")\n            results[model] = {\"setup\": None, \"query\": None}\n\n    # Summary\n    print(f\"\\n{'='*60}\")\n    print(\"\ud83d\udcca PERFORMANCE SUMMARY\")\n    print(f\"{'='*60}\")\n    print(f\"{'Model':&lt;25} {'Dimensions':&lt;12} {'Setup':&lt;10} {'Query':&lt;10}\")\n    print(\"-\" * 60)\n\n    model_dims = {\"bge-base-en-v1.5\": 768, \"bge-m3\": 1024, \"bge-multilingual-gemma2\": 3584}\n\n    for model, times in results.items():\n        dims = model_dims[model]\n        setup = f\"{times['setup']:.2f}s\" if times['setup'] else \"Failed\"\n        query = f\"{times['query']:.2f}s\" if times['query'] else \"Failed\"\n        print(f\"{model:&lt;25} {dims:&lt;12} {setup:&lt;10} {query:&lt;10}\")\n\n    print(f\"\\n\ud83d\udca1 Performance Insights:\")\n    print(\"   \u2022 Higher dimensions = better accuracy, slower performance\")\n    print(\"   \u2022 Lower dimensions = faster performance, potentially less accuracy\")\n    print(\"   \u2022 Choose based on your speed vs accuracy requirements\")\n\nif __name__ == \"__main__\":\n    test_performance()\n</code></pre> <p>Run the performance test:</p> <pre><code>python performance_test.py\n</code></pre> <p>Expected output:</p> <pre><code>\ud83d\ude80 Starting comprehensive performance test...\n\n\ud83e\uddea Testing bge-base-en-v1.5...\n\u23f1\ufe0f  Setup time: 0.85 seconds\n\u23f1\ufe0f  Query time: 1.45 seconds\n\ud83d\udcac Answer: [LLM response here]\n\n\ud83d\udcca PERFORMANCE SUMMARY\n============================================================\nModel                     Dimensions   Setup      Query     \n------------------------------------------------------------\nbge-base-en-v1.5         768          0.85s      1.45s     \nbge-m3                   1024         0.95s      1.60s     \nbge-multilingual-gemma2  3584         1.16s      1.99s\n</code></pre>"},{"location":"public-cloud/ai-endpoints/rag-tutorial/setup-guide/#step-11-test-ovhcloud-documentation-examples","title":"Step 11: Test OVHcloud Documentation Examples","text":"<p>Create and run a test that matches OVHcloud's official documentation:</p> <pre><code>nano test_ovh_docs_example.py\n</code></pre> <pre><code>import os\nimport requests\nimport numpy as np\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\ndef test_ovh_docs_examples():\n    \"\"\"Test the exact examples from OVHcloud documentation\"\"\"\n\n    # Test data from OVHcloud docs\n    text = [\n        \"Paris is the capital of France\",\n        \"Paris is the capital of France\", \n        \"Berlin is the capital of Germany\",\n        \"This endpoint converts input sentence into a vector embeddings\"\n    ]\n\n    headers = {\n        \"Content-Type\": \"text/plain\",\n        \"Authorization\": f\"Bearer {os.getenv('OVH_AI_ENDPOINTS_ACCESS_TOKEN')}\",\n    }\n\n    # Test all three models from documentation\n    models = [\n        (\"bge-multilingual-gemma2\", \"https://bge-multilingual-gemma2.endpoints.kepler.ai.cloud.ovh.net/api/text2vec\"),\n        (\"bge-m3\", \"https://bge-m3.endpoints.kepler.ai.cloud.ovh.net/api/text2vec\"),\n        (\"bge-base-en-v1.5\", \"https://bge-base-en-v1-5.endpoints.kepler.ai.cloud.ovh.net/api/text2vec\")\n    ]\n\n    # Cosine similarity function from docs\n    def cosine_similarity(vec_source, vec_compare):\n        return np.dot(vec_source, vec_compare)\n\n    for model_name, url in models:\n        print(f\"\\n\ud83e\uddea Testing {model_name}...\")\n        print(f\"URL: {url}\")\n\n        response_data = []\n        sentence_similarity = {}\n\n        for s in range(len(text)):\n            # Generate embeddings vector\n            response = requests.post(url, data=text[s], headers=headers)\n            if response.status_code == 200:\n                response_data.append(response.json())\n                if s &gt; 0:\n                    # Calculate sentence similarity\n                    similarity = cosine_similarity(response_data[0], response_data[s])\n                    sentence_similarity[f\"Similarity with sentence n\u00b0{s}\"] = \"{:.3f}\".format(similarity)\n            else:\n                print(f\"\u274c Error: {response.status_code}\")\n                print(f\"Response: {response.text}\")\n                break\n\n        if response_data:\n            print(f\"\u2705 {model_name} working!\")\n            print(f\"\ud83d\udcca Embedding dimensions: {len(response_data[0])}\")\n            print(f\"\ud83d\udd0d Similarities: {sentence_similarity}\")\n            return model_name, url  # Return first working model\n        else:\n            print(f\"\u274c {model_name} failed\")\n\n    return None, None\n\nif __name__ == \"__main__\":\n    print(\"Testing OVHcloud documentation examples...\")\n    working_model, working_url = test_ovh_docs_examples()\n\n    if working_model:\n        print(f\"\\n\ud83c\udf89 Success! Use {working_model} for RAG testing.\")\n        print(f\"Working URL: {working_url}\")\n    else:\n        print(\"\\n\u274c All models failed. Check your token.\")\n</code></pre> <p>Run this test:</p> <pre><code>python test_ovh_docs_example.py\n</code></pre> <p>You should see output showing successful embedding generation and similarity calculations.</p>"},{"location":"public-cloud/ai-endpoints/rag-tutorial/setup-guide/#step-12-experiment-and-customize","title":"Step 12: Experiment and Customize","text":"<p>Now that everything is working, try customizing the scripts to deepen your understanding:</p>"},{"location":"public-cloud/ai-endpoints/rag-tutorial/setup-guide/#a-test-different-knowledge-bases","title":"A. Test Different Knowledge Bases","text":"<p>Edit <code>test_rag_ovh.py</code> and replace the chunks with your own content:</p> <pre><code>nano test_rag_ovh.py\n</code></pre> <p>Find the <code>chunks = [...]</code> section and replace with different knowledge:</p> <pre><code>chunks = [\n    Document(\n        page_content=\"OVHcloud is a European cloud provider that keeps your data sovereign - meaning your data stays where you put it, not scattered across unknown servers in foreign countries. Perfect for companies who like to know where their bits and bytes are vacationing!\",\n        metadata={\"source\": \"ovh_facts\", \"chunk_id\": 1}\n    ),\n    Document(\n        page_content=\"OVHcloud offers predictable pricing with no surprise bills at month-end. Unlike some cloud providers who charge you for breathing near their servers, OVHcloud tells you upfront what you'll pay. It's like having a honest mechanic, but for cloud computing!\",\n        metadata={\"source\": \"ovh_facts\", \"chunk_id\": 2}\n    ),\n    Document(\n        page_content=\"OVHcloud uses water cooling in their standard server design, making their data centers more efficient than air-cooled alternatives. They're literally keeping it cool while keeping costs down - talk about a refreshing approach to cloud infrastructure!\",\n        metadata={\"source\": \"ovh_facts\", \"chunk_id\": 3}\n    ),\n    Document(\n        page_content=\"OVHcloud's Go Global initiative spans 4 continents with data centers worldwide, yet maintains local presence and support. They're globally distributed but locally rooted - like a multinational company that still remembers your birthday!\",\n        metadata={\"source\": \"ovh_facts\", \"chunk_id\": 4}\n    )\n]\n</code></pre>"},{"location":"public-cloud/ai-endpoints/rag-tutorial/setup-guide/#b-try-different-questions","title":"B. Try Different Questions","text":"<p>Modify the <code>test_queries</code> list to ask questions relevant to your new knowledge base:</p> <pre><code>test_queries = [\n    \"What makes OVHcloud different for data sovereignty?\",\n    \"How does OVHcloud keep pricing predictable?\",\n    \"Tell me about OVHcloud's cooling technology\",\n    \"How do I bake a perfect souffl\u00e9?\"  # Irrelevant question to test boundaries\n]\n</code></pre>"},{"location":"public-cloud/ai-endpoints/rag-tutorial/setup-guide/#c-test-different-models","title":"C. Test Different Models","text":"<p>Create a quick model comparison script:</p> <pre><code>nano model_comparison.py\n</code></pre> <pre><code>from test_rag_ovh import OVHEmbeddings\nimport requests\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\ndef compare_embeddings():\n    \"\"\"Compare how different models embed the same text\"\"\"\n    test_text = \"OVHcloud provides data sovereignty with predictable pricing and innovative water cooling technology.\"\n\n    models = [\"bge-base-en-v1.5\", \"bge-m3\", \"bge-multilingual-gemma2\"]\n\n    for model in models:\n        try:\n            embeddings = OVHEmbeddings(model_name=model)\n            result = embeddings.embed_query(test_text)\n            print(f\"{model}: {len(result)} dimensions, first 3 values: {result[:3]}\")\n        except Exception as e:\n            print(f\"{model}: Failed - {e}\")\n\nif __name__ == \"__main__\":\n    compare_embeddings()\n</code></pre> <pre><code>python model_comparison.py\n</code></pre>"},{"location":"public-cloud/ai-endpoints/rag-tutorial/setup-guide/#d-experiment-with-similarity-thresholds","title":"D. Experiment with Similarity Thresholds","text":"<p>Create a similarity experiment:</p> <pre><code>nano similarity_experiment.py\n</code></pre> <pre><code>from langchain_community.vectorstores import FAISS\nfrom langchain.schema import Document\nfrom test_rag_ovh import OVHEmbeddings\n\ndef test_similarity_thresholds():\n    \"\"\"Experiment with different similarity approaches\"\"\"\n\n    # Create test documents about cloud providers\n    docs = [\n        Document(page_content=\"OVHcloud ensures data sovereignty by keeping your data in specific geographic regions with transparent policies\"),\n        Document(page_content=\"AWS offers global cloud services with pay-as-you-use pricing across multiple availability zones\"), \n        Document(page_content=\"Cats are domestic animals that make popular pets and enjoy playing with yarn\"),\n        Document(page_content=\"Dogs are loyal companion animals known for their intelligence and trainability\")\n    ]\n\n    embeddings = OVHEmbeddings(model_name=\"bge-base-en-v1.5\")\n    vectorstore = FAISS.from_documents(docs, embeddings)\n\n    query = \"Tell me about cloud data sovereignty\"\n\n    # Test different k values\n    for k in [1, 2, 3, 4]:\n        results = vectorstore.similarity_search(query, k=k)\n        print(f\"\\nTop {k} results for '{query}':\")\n        for i, doc in enumerate(results):\n            print(f\"  {i+1}. {doc.page_content}\")\n\nif __name__ == \"__main__\":\n    test_similarity_thresholds()\n</code></pre> <pre><code>python similarity_experiment.py\n</code></pre>"},{"location":"public-cloud/ai-endpoints/rag-tutorial/setup-guide/#e-create-your-own-rag-application","title":"E. Create Your Own RAG Application","text":"<p>Try building a simple RAG app for a cooking assistant:</p> <pre><code>nano my_rag_app.py\n</code></pre> <p>```python from test_rag_ovh import OVHEmbeddings, OVHLLM, Document from langchain_community.vectorstores import FAISS from langchain.chains import RetrievalQA</p> <p>def create_ovhcloud_rag():     \"\"\"Create a RAG system for OVHcloud information\"\"\"</p> <pre><code># OVHcloud knowledge base with fun facts\novhcloud_knowledge = [\n    Document(\n        page_content=\"OVHcloud pioneered using water cooling in standard server designs, making data centers 30% more energy efficient than traditional air cooling. They're literally keeping it cool while saving the planet - one server at a time!\",\n        metadata={\"source\": \"ovh_tech\", \"chunk_id\": 1}\n    ),\n    Document(\n        page_content=\"OVHcloud's pay-as-you-go model means you only pay for what you actually use, with per-second billing on many services. No more guessing or overpaying - just transparent, predictable cloud costs that won't give your finance team a heart attack.\",\n        metadata={\"source\": \"ovh_pricing\", \"chunk_id\": 2}\n    ),\n    Document(\n        page_content=\"OVHclou\n</code></pre>"},{"location":"public-cloud/ai-endpoints/vlm-tutorial-car-damage-verfication/","title":"VLM Tutorial - Car Damage Verification","text":"<p>Build an interactive AI-powered car verification system using OVHcloud Vision Language Models.</p>"},{"location":"public-cloud/ai-endpoints/vlm-tutorial-car-damage-verfication/#what-youll-build","title":"What You'll Build","text":"<p>An interactive car verification challenge that: - Collects user claims about their vehicle (make, model, color, damage) - Analyzes uploaded photos using Vision Language Models - Cross-references visual evidence with textual claims - Provides detailed verification reports with confidence levels - Demonstrates multi-modal AI capabilities</p>"},{"location":"public-cloud/ai-endpoints/vlm-tutorial-car-damage-verfication/#learning-path","title":"Learning Path","text":"<ol> <li>Setup Guide - Complete step-by-step tutorial</li> <li>Download Scripts - All Python files</li> </ol>"},{"location":"public-cloud/ai-endpoints/vlm-tutorial-car-damage-verfication/#demo-overview","title":"Demo Overview","text":"<p>The Car Verification Challenge is an interactive fact-checking experiment where users:</p> <ol> <li>Make claims about their car (manufacturer, model, color, damage)</li> <li>Upload photos of the actual vehicle (minimum 3 photos)</li> <li>Get AI analysis that verifies if the photos match their claims</li> <li>Receive a verdict - truth or deception detected!</li> </ol>"},{"location":"public-cloud/ai-endpoints/vlm-tutorial-car-damage-verfication/#key-features","title":"Key Features","text":"<ul> <li>Multi-modal AI - Combines text and image analysis</li> <li>Interactive interface - User-friendly Chainlit web application</li> <li>Real-time verification - Live AI-powered fact-checking</li> <li>Educational demonstrations - Learn about AI vision capabilities</li> <li>Practical applications - Insurance, automotive, verification use cases</li> </ul>"},{"location":"public-cloud/ai-endpoints/vlm-tutorial-car-damage-verfication/#quick-start","title":"Quick Start","text":"<p>Before You Begin</p> <p>Make sure you have: - OVHcloud account with AI Endpoints access - Basic Python knowledge - Car photos for testing (at least 3 images) - Linux/macOS/Windows environment</p> <p>Start Tutorial Download Scripts</p>"},{"location":"public-cloud/ai-endpoints/vlm-tutorial-car-damage-verfication/#technology-stack","title":"Technology Stack","text":"<ul> <li>Vision Model: Qwen2.5-VL-72B-Instruct</li> <li>Provider: OVHcloud AI Endpoints</li> <li>Interface: Chainlit web application</li> <li>Image Processing: Pillow (PIL)</li> <li>API Communication: Python requests</li> </ul>"},{"location":"public-cloud/ai-endpoints/vlm-tutorial-car-damage-verfication/#educational-value","title":"Educational Value","text":"<p>This tutorial demonstrates: - Vision Language Model integration - Practical VLM implementation - Multi-modal prompting - Combining text and image inputs - Verification systems - AI-powered fact-checking - Interactive AI applications - Building engaging user experiences - Real-world use cases - Insurance, automotive, verification scenarios</p>"},{"location":"public-cloud/ai-endpoints/vlm-tutorial-car-damage-verfication/#difficulty-level","title":"Difficulty Level","text":"<p>Intermediate Time: 1-2 hours Prerequisites: Basic Python, OVHcloud account</p>"},{"location":"public-cloud/ai-endpoints/vlm-tutorial-car-damage-verfication/setup-guide/","title":"VLM Tutorial - Car Verification Setup Guide","text":"<p>About this guide</p> <p>This step-by-step tutorial will walk you through testing the AI car verification demo using OVHcloud's Vision Language Models. Follow each step carefully to explore AI vision capabilities.</p>"},{"location":"public-cloud/ai-endpoints/vlm-tutorial-car-damage-verfication/setup-guide/#prerequisites","title":"Prerequisites","text":"<ul> <li> OVHcloud account with AI Endpoints access</li> <li> Basic Python knowledge</li> <li> Car photos for testing (at least 3 different images)</li> <li> Linux/macOS/Windows environment</li> </ul>"},{"location":"public-cloud/ai-endpoints/vlm-tutorial-car-damage-verfication/setup-guide/#step-1-system-setup-and-python-installation","title":"Step 1: System Setup and Python Installation","text":"<p>System Requirements</p> <p>This demo requires Python 3.11+ and pip. Choose your operating system below.</p> Debian/UbuntuCentOS/RHELmacOSWindows <pre><code># Update system\nsudo apt update &amp;&amp; sudo apt upgrade -y\n\n# Install Python 3.11+ and dependencies\nsudo apt install python3 python3-pip python3-venv curl -y\n\n# Verify Python version\npython3 --version\n</code></pre> <pre><code># Update system\nsudo yum update -y\n\n# Install Python and dependencies\nsudo yum install python3 python3-pip python3-venv curl -y\n\n# Verify installation\npython3 --version\n</code></pre> <pre><code># Install Homebrew if needed\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\n# Install Python 3.11+\nbrew install python@3.11\n\n# Verify installation\npython3 --version\n</code></pre> <pre><code># Download Python from python.org and install\n# Or use Windows Package Manager\nwinget install Python.Python.3.11\n\n# Verify installation\npython --version\n</code></pre>"},{"location":"public-cloud/ai-endpoints/vlm-tutorial-car-damage-verfication/setup-guide/#step-2-get-ovhcloud-ai-endpoints-access-token","title":"Step 2: Get OVHcloud AI Endpoints Access Token","text":"<p>Vision Model Access Required</p> <p>This demo requires access to OVHcloud's Qwen2.5-VL-72B-Instruct model. You'll need an AI Endpoints access token.</p>"},{"location":"public-cloud/ai-endpoints/vlm-tutorial-car-damage-verfication/setup-guide/#token-acquisition-steps","title":"Token Acquisition Steps","text":"<ul> <li> Go to OVHcloud AI Endpoints</li> <li> Create account or sign in</li> <li> Navigate to Public Cloud dashboard</li> <li> Create or select existing Public Cloud Project</li> <li> Go to AI Endpoints \u2192 API keys</li> <li> Click Create a new API key</li> <li> Copy the generated token</li> </ul> <p>Token Storage</p> <p>Keep your token secure - we'll store it in environment variables for safety.</p> <pre><code>flowchart TD\n    A[Start] --&gt; B[OVHcloud Account]\n    B --&gt; C[Public Cloud Project]\n    C --&gt; D[AI Endpoints Section]\n    D --&gt; E[Create API Key]\n    E --&gt; F[Copy Token]\n    F --&gt; G[Use in Demo]\n    style E fill:#f96,stroke:#333,stroke-width:2px\n    style F fill:#f96,stroke:#333,stroke-width:2px</code></pre>"},{"location":"public-cloud/ai-endpoints/vlm-tutorial-car-damage-verfication/setup-guide/#step-3-create-demo-environment","title":"Step 3: Create Demo Environment","text":"<pre><code># Create demo directory\nmkdir ~/car-verification-demo\ncd ~/car-verification-demo\n\n# Create Python virtual environment\npython3 -m venv venv\n\n# Activate virtual environment\nsource venv/bin/activate  # Linux/macOS\n# OR for Windows: venv\\Scripts\\activate\n\n# Upgrade pip\npip install --upgrade pip\n</code></pre>"},{"location":"public-cloud/ai-endpoints/vlm-tutorial-car-damage-verfication/setup-guide/#step-4-download-requirements-file","title":"Step 4: Download Requirements File","text":"<p>Download the requirements file from the repository:</p> <pre><code># Download requirements.txt\ncurl -o requirements.txt https://raw.githubusercontent.com/cougz/ovhcloud-workbooks/main/public-cloud/ai-endpoints/vlm-tutorial-car-damage-verfication/requirements.txt\n\n# Install all dependencies\npip install -r requirements.txt\n\n# Verify installations\npip list | grep -E \"(chainlit|pillow|requests)\"\n</code></pre> <p>You should see output showing the installed packages and their versions.</p>"},{"location":"public-cloud/ai-endpoints/vlm-tutorial-car-damage-verfication/setup-guide/#step-5-configure-environment-variables","title":"Step 5: Configure Environment Variables","text":"<pre><code># Create environment configuration file\nnano .env\n</code></pre> <p>Add your OVHcloud token:</p> <pre><code># OVHcloud AI Endpoints Configuration\nOVH_AI_ENDPOINTS_ACCESS_TOKEN=your_actual_token_here\n\n# AI Model Endpoint\nQWEN_URL=https://qwen-2-5-vl-72b-instruct.endpoints.kepler.ai.cloud.ovh.net/api/openai_compat/v1/chat/completions\n</code></pre>"},{"location":"public-cloud/ai-endpoints/vlm-tutorial-car-damage-verfication/setup-guide/#step-6-download-tutorial-files","title":"Step 6: Download Tutorial Files","text":"<p>Download all the required Python files from the repository:</p> Option 1: Download Individual FilesOption 2: Clone RepositoryOption 3: Manual Download <p>Download each file individually from GitHub:</p> <pre><code># Download all required files\ncurl -o requirements.txt https://raw.githubusercontent.com/cougz/ovhcloud-workbooks/main/public-cloud/ai-endpoints/vlm-tutorial-car-damage-verfication/requirements.txt\ncurl -o test_vision_connection.py https://raw.githubusercontent.com/cougz/ovhcloud-workbooks/main/public-cloud/ai-endpoints/vlm-tutorial-car-damage-verfication/test_vision_connection.py\ncurl -o verification_demo.py https://raw.githubusercontent.com/cougz/ovhcloud-workbooks/main/public-cloud/ai-endpoints/vlm-tutorial-car-damage-verfication/verification_demo.py\ncurl -o verification_app.py https://raw.githubusercontent.com/cougz/ovhcloud-workbooks/main/public-cloud/ai-endpoints/vlm-tutorial-car-damage-verfication/verification_app.py\ncurl -o chainlit.md https://raw.githubusercontent.com/cougz/ovhcloud-workbooks/main/public-cloud/ai-endpoints/vlm-tutorial-car-damage-verfication/chainlit.md\n</code></pre> <p>Clone the entire repository and copy the files:</p> <pre><code># Clone the repository\ngit clone https://github.com/cougz/ovhcloud-workbooks.git\n\n# Copy VLM tutorial files to your demo directory\ncp ovhcloud-workbooks/public-cloud/ai-endpoints/vlm-tutorial-car-damage-verfication/requirements.txt .\ncp ovhcloud-workbooks/public-cloud/ai-endpoints/vlm-tutorial-car-damage-verfication/*.py .\ncp ovhcloud-workbooks/public-cloud/ai-endpoints/vlm-tutorial-car-damage-verfication/chainlit.md .\n\n# Clean up\nrm -rf ovhcloud-workbooks\n</code></pre> <p>Visit the GitHub repository and download files manually:</p> <ol> <li>Go to VLM Tutorial Files</li> <li>Click on each file and download:</li> <li><code>requirements.txt</code></li> <li><code>test_vision_connection.py</code></li> <li><code>verification_demo.py</code> </li> <li><code>verification_app.py</code></li> <li><code>chainlit.md</code></li> <li>Save them to your <code>~/car-verification-demo</code> directory</li> </ol> <p>File Contents</p> <p>You can also view the complete source code for each file in the repository to understand how the VLM verification system works.</p>"},{"location":"public-cloud/ai-endpoints/vlm-tutorial-car-damage-verfication/setup-guide/#step-7-test-ovhcloud-vision-api-connectivity","title":"Step 7: Test OVHcloud Vision API Connectivity","text":"<pre><code># Test the vision API connection\npython test_vision_connection.py\n</code></pre> <p>Expected output:</p> <pre><code>Testing OVHcloud Vision API connectivity...\n\n\ud83d\udd0d Testing OVHcloud Vision API...\n\u2705 Vision API works!\n\ud83e\udd16 AI Response: I can see a simple drawing of a red car with black wheels on a light blue background. There's text that says \"TEST CAR\" at the top of the image.\n\n\ud83c\udf89 Vision API is working! Ready for demo testing.\n</code></pre>"},{"location":"public-cloud/ai-endpoints/vlm-tutorial-car-damage-verfication/setup-guide/#step-8-launch-the-interactive-demo","title":"Step 8: Launch the Interactive Demo","text":"<p>Now that you have all the files, you can run the car verification demo:</p> <pre><code># First, test the API connection\npython test_vision_connection.py\n\n# If successful, run the interactive demo\nchainlit run verification_app.py --host 0.0.0.0 --port 8000\n</code></pre> <p>You should see:</p> <pre><code>2025-06-03 09:39:15 - Loaded .env file\n2025-06-03 09:39:16 - Your app is available at http://localhost:8000\n</code></pre> <p>Network Access</p> <p>The demo is accessible from any device on your network via <code>http://YOUR_SERVER_IP:8000</code></p>"},{"location":"public-cloud/ai-endpoints/vlm-tutorial-car-damage-verfication/setup-guide/#step-9-demo-testing-scenarios","title":"Step 9: Demo Testing Scenarios","text":""},{"location":"public-cloud/ai-endpoints/vlm-tutorial-car-damage-verfication/setup-guide/#scenario-a-truth-test","title":"Scenario A: Truth Test","text":"<p>Test honest claims to see AI verification:</p> <ol> <li>Access the demo: Open http://localhost:8000 in your browser</li> <li>Enter true details:</li> <li>Manufacturer: Toyota</li> <li>Model: Camry</li> <li>Color: White</li> <li>Damage: Scratch on front bumper</li> <li>Upload matching photos: 3 photos of an actual white Toyota Camry with visible front damage</li> <li>Observe results: AI should confirm all claims match</li> </ol>"},{"location":"public-cloud/ai-endpoints/vlm-tutorial-car-damage-verfication/setup-guide/#scenario-b-deception-detection-test","title":"Scenario B: Deception Detection Test","text":"<p>Test if AI catches false claims:</p> <ol> <li>Enter false details:</li> <li>Manufacturer: BMW</li> <li>Model: 4 Series</li> <li>Color: Black</li> <li>Damage: No damage</li> <li>Upload different car: Photos of a white Honda Civic with visible dents</li> <li>Watch AI detect lies: Should catch all false claims</li> </ol>"},{"location":"public-cloud/ai-endpoints/vlm-tutorial-car-damage-verfication/setup-guide/#scenario-c-partial-truth-test","title":"Scenario C: Partial Truth Test","text":"<p>Test mixed true/false claims:</p> <ol> <li>Enter mixed details:</li> <li>Manufacturer: Honda (correct)</li> <li>Model: Civic (correct)</li> <li>Color: Blue (correct)</li> <li>Damage: No damage (false - car has rear damage)</li> <li>Upload photos: Blue Honda Civic with hidden rear damage</li> <li>See AI spot omission: Should verify car details but catch undisclosed damage</li> </ol>"},{"location":"public-cloud/ai-endpoints/vlm-tutorial-car-damage-verfication/setup-guide/#expected-demo-results","title":"Expected Demo Results","text":"<p>\u2705 Successful Demo Run:</p> <ul> <li>API connectivity test passes</li> <li>Web interface loads at localhost:8000</li> <li>AI processes images in 30-60 seconds</li> <li>Verification reports show detailed analysis</li> <li>AI catches discrepancies between claims and photos</li> </ul> <p>\u2705 Learning Outcomes:</p> <ul> <li>Understanding of AI vision capabilities</li> <li>Experience with prompt engineering</li> <li>Knowledge of API integration patterns</li> <li>Realistic expectations for AI accuracy</li> </ul>"},{"location":"public-cloud/ai-endpoints/vlm-tutorial-car-damage-verfication/setup-guide/#troubleshooting","title":"Troubleshooting","text":""},{"location":"public-cloud/ai-endpoints/vlm-tutorial-car-damage-verfication/setup-guide/#common-issues","title":"Common Issues","text":"<p>Connection Errors: <pre><code># Test your token\npython test_vision_connection.py\n</code></pre></p> <p>Image Upload Problems: - Check file formats (PNG, JPG, JPEG, WebP supported) - Reduce image sizes if upload fails - Ensure at least 3 photos for verification</p> <p>Slow Performance: - Reduce image resolution - Check network connection - Wait for processing (30-60 seconds normal)</p> <p>Token Errors: - Verify OVHcloud AI Endpoints token in <code>.env</code> - Check token permissions and expiration - Regenerate token if needed</p>"},{"location":"public-cloud/ai-endpoints/vlm-tutorial-car-damage-verfication/setup-guide/#clean-up","title":"Clean Up","text":"<pre><code># Deactivate virtual environment when done\ndeactivate\n\n# Optional: Save your experiments\nmkdir results\n# Save any generated verification reports\n\n# Optional: Remove demo directory\n# rm -rf ~/car-verification-demo\n</code></pre>"},{"location":"public-cloud/ai-endpoints/vlm-tutorial-car-damage-verfication/setup-guide/#quick-start-commands-summary","title":"Quick Start Commands Summary","text":"<pre><code># Complete demo setup (copy-paste friendly)\nmkdir ~/car-verification-demo &amp;&amp; cd ~/car-verification-demo &amp;&amp; \\\npython3 -m venv venv &amp;&amp; source venv/bin/activate\n\n# Download all tutorial files\ncurl -o requirements.txt https://raw.githubusercontent.com/cougz/ovhcloud-workbooks/main/public-cloud/ai-endpoints/vlm-tutorial-car-damage-verfication/requirements.txt\ncurl -o test_vision_connection.py https://raw.githubusercontent.com/cougz/ovhcloud-workbooks/main/public-cloud/ai-endpoints/vlm-tutorial-car-damage-verfication/test_vision_connection.py\ncurl -o verification_demo.py https://raw.githubusercontent.com/cougz/ovhcloud-workbooks/main/public-cloud/ai-endpoints/vlm-tutorial-car-damage-verfication/verification_demo.py\ncurl -o verification_app.py https://raw.githubusercontent.com/cougz/ovhcloud-workbooks/main/public-cloud/ai-endpoints/vlm-tutorial-car-damage-verfication/verification_app.py\ncurl -o chainlit.md https://raw.githubusercontent.com/cougz/ovhcloud-workbooks/main/public-cloud/ai-endpoints/vlm-tutorial-car-damage-verfication/chainlit.md\n\n# Install dependencies\npip install -r requirements.txt\n\n# Create .env with your token\necho \"OVH_AI_ENDPOINTS_ACCESS_TOKEN=your_token_here\" &gt; .env\necho \"QWEN_URL=https://qwen-2-5-vl-72b-instruct.endpoints.kepler.ai.cloud.ovh.net/api/openai_compat/v1/chat/completions\" &gt;&gt; .env\n\n# Test connection\npython test_vision_connection.py\n\n# Run the demo\nchainlit run verification_app.py --host 0.0.0.0 --port 8000\n</code></pre> <p>This demo successfully demonstrates OVHcloud's Vision Language Model capabilities for practical verification tasks while maintaining clear educational boundaries.</p>"}]}